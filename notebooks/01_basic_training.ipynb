{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Drone Training with PPO\n",
    "\n",
    "This notebook demonstrates how to train a drone using PPO algorithm.\n",
    "\n",
    "## Quick Start\n",
    "1. Import training functions\n",
    "2. Configure training parameters\n",
    "3. Train the model\n",
    "4. Test the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path if needed\n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "    \n",
    "from crazy_flie_env import CrazyFlieEnv\n",
    "import numpy as np\n",
    "\n",
    "env = CrazyFlieEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path if needed\n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Import training utilities\n",
    "from train.simple_train import train_ppo, train_sac, load_model\n",
    "from train.config import TrainingConfig\n",
    "from train.test_utils import test_model, quick_test, visualize_episode\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick Test (Optional)\n",
    "\n",
    "Test the environment before training to make sure everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Dict('image': Box(0, 255, (720, 1280, 3), uint8), 'state': Box([-10.        -10.          0.         -5.         -5.         -5.\n",
      "  -3.1415927  -3.1415927  -3.1415927 -10.        -10.        -10.       ], [10.        10.        10.         5.         5.         5.\n",
      "  3.1415927  3.1415927  3.1415927 10.        10.        10.       ], (12,), float32))\n",
      "Action space: Box([-1. -1. -1.  0.], 1.0, (4,), float32)\n",
      "State shape: (12,)\n",
      "Image shape: (720, 1280, 3)\n",
      "Step completed! Reward: 0.121\n",
      "\n",
      "‚úÖ Environment test passed!\n"
     ]
    }
   ],
   "source": [
    "# Quick environment test\n",
    "from crazy_flie_env import CrazyFlieEnv\n",
    "\n",
    "env = CrazyFlieEnv()\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"State shape: {obs['state'].shape}\")\n",
    "print(f\"Image shape: {obs['image'].shape}\")\n",
    "\n",
    "# Test one step\n",
    "action = env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "print(f\"Step completed! Reward: {reward:.3f}\")\n",
    "\n",
    "env.close()\n",
    "print(\"\\n‚úÖ Environment test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Training\n",
    "\n",
    "Create a training configuration. You can modify any parameters here.\n",
    "\n",
    "### Live Visualization Options:\n",
    "- Set `render_during_training=True` to see the MuJoCo viewer during training\n",
    "- Set `render_freq` to control how often to render (lower = more frequent, but slower training)\n",
    "- Set `show_live_metrics=True` to see training metrics in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Algorithm: PPO\n",
      "  Total timesteps: 100,000\n",
      "  Parallel envs: 1\n",
      "  Learning rate: 0.0003\n",
      "  Device: cpu\n",
      "  Live rendering: True\n",
      "  Render frequency: every 500 steps\n"
     ]
    }
   ],
   "source": [
    "# Quick training (for testing) - WITHOUT live visualization\n",
    "# config = TrainingConfig(\n",
    "#     algorithm=\"PPO\",\n",
    "#     total_timesteps=100_000,  \n",
    "#     num_envs=1,\n",
    "#     learning_rate=3e-4,\n",
    "#     n_steps=2048,\n",
    "#     batch_size=64,\n",
    "#     eval_freq=10_000,\n",
    "#     save_freq=20_000,\n",
    "#     render_during_training=True,  # Set to True to see live MuJoCo viewer\n",
    "#     show_live_metrics=False\n",
    "# )\n",
    "\n",
    "# Alternative: Training WITH live visualization (slower but you can watch!)\n",
    "config = TrainingConfig(\n",
    "    algorithm=\"PPO\",\n",
    "    total_timesteps=100_000,  \n",
    "    num_envs=1,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    eval_freq=10_000,\n",
    "    save_freq=20_000,\n",
    "    render_during_training=True,   # Enable live viewer\n",
    "    render_freq=10,                # Render every 500 steps\n",
    "    show_live_metrics=True,         # Show metrics during training\n",
    "    metrics_freq=2000               # Print metrics every 2000 steps\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Algorithm: {config.algorithm}\")\n",
    "print(f\"  Total timesteps: {config.total_timesteps:,}\")\n",
    "print(f\"  Parallel envs: {config.num_envs}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Device: {config.device}\")\n",
    "print(f\"  Live rendering: {config.render_during_training}\")\n",
    "if config.render_during_training:\n",
    "    print(f\"  Render frequency: every {config.render_freq} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train PPO Agent\n",
    "\n",
    "This will train the agent. Progress bar will show training progress.\n",
    "\n",
    "### Live Visualization\n",
    "If you enabled `render_during_training=True`, the MuJoCo viewer window will pop up during training, allowing you to watch the drone learn in real-time!\n",
    "\n",
    "‚ö†Ô∏è **Note**: \n",
    "- Training can take a while depending on `total_timesteps`\n",
    "- Live rendering will slow down training but lets you watch the learning process\n",
    "- 100K steps: ~10-30 minutes (without rendering), ~20-45 minutes (with rendering)\n",
    "- 500K steps: ~1-2 hours (without rendering)\n",
    "- 1M steps: ~2-4 hours (without rendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting PPO Training\n",
      "   Timesteps: 100,000\n",
      "   Environments: 1\n",
      "   Device: cpu\n",
      "üìÅ Model directory: models/ppo_drone_20251001_151324\n",
      "üìÅ Log directory: logs/ppo_drone_20251001_151324\n",
      "‚úÖ Model loaded: C:\\Users\\Ratan.Bunkar\\Learning\\general\\rl-agent\\Drone-UAV\\bitcraze_crazyflie_2\\scene.xml\n",
      "üìä Model info: 2 bodies, 7 DOF\n",
      "‚úÖ Found drone body ID: 1\n",
      "‚úÖ Drone controller initialized\n",
      "üéõÔ∏è Controller gains:\n",
      "   Altitude: Kp=0.275, Ki=0.022, Kd=0.108\n",
      "   Roll: Kp=0.325367\n",
      "   Yaw: Kp=0.219712\n",
      "‚úÖ Observation space defined:\n",
      "   State vector: 12 dimensions\n",
      "   Camera image: (720, 1280, 3)\n",
      "‚úÖ Action space defined:\n",
      "   Roll command: [-1.0, 1.0]\n",
      "   Pitch command: [-1.0, 1.0]\n",
      "   Yaw rate command: [-1.0, 1.0]\n",
      "   Thrust command: [0.0, 1.0]\n",
      "‚úÖ Found drone FPV camera\n",
      "üì∑ Available cameras:\n",
      "   Camera 0: track\n",
      "‚úÖ Created 3 renderers\n",
      "‚úÖ Camera system initialized\n",
      "üì∑ Camera system status:\n",
      "   drone_fpv: ID=-1, pos=[-1.00, 0.00, 0.50]\n",
      "‚úÖ Rendering system initialized (OpenCV: ‚úÖ)\n",
      "‚úÖ Reward calculator initialized\n",
      "üéØ Reward components:\n",
      "   height_maintenance: weight=-0.100 - Reward for maintaining target altitude\n",
      "   stability: weight=-0.010 - Reward for low angular velocities\n",
      "   action_penalty: weight=-0.001 - Penalty for large control actions\n",
      "   crash_penalty: weight=1.000 - Large penalty for crashing\n",
      "   progress: weight=0.100 - Reward for forward progress\n",
      "   efficiency: weight=0.050 - Bonus for efficient movement\n",
      "‚úÖ CrazyFlie Environment initialized\n",
      "üìä Observation space: Dict('image': Box(0, 255, (720, 1280, 3), uint8), 'state': Box([-10.        -10.          0.         -5.         -5.         -5.\n",
      "  -3.1415927  -3.1415927  -3.1415927 -10.        -10.        -10.       ], [10.        10.        10.         5.         5.         5.\n",
      "  3.1415927  3.1415927  3.1415927 10.        10.        10.       ], (12,), float32))\n",
      "üéÆ Action space: Box([-1. -1. -1.  0.], 1.0, (4,), float32)\n",
      "‚úÖ Model loaded: C:\\Users\\Ratan.Bunkar\\Learning\\general\\rl-agent\\Drone-UAV\\bitcraze_crazyflie_2\\scene.xml\n",
      "üìä Model info: 2 bodies, 7 DOF\n",
      "‚úÖ Found drone body ID: 1\n",
      "‚úÖ Drone controller initialized\n",
      "üéõÔ∏è Controller gains:\n",
      "   Altitude: Kp=0.275, Ki=0.022, Kd=0.108\n",
      "   Roll: Kp=0.325367\n",
      "   Yaw: Kp=0.219712\n",
      "‚úÖ Observation space defined:\n",
      "   State vector: 12 dimensions\n",
      "   Camera image: (720, 1280, 3)\n",
      "‚úÖ Action space defined:\n",
      "   Roll command: [-1.0, 1.0]\n",
      "   Pitch command: [-1.0, 1.0]\n",
      "   Yaw rate command: [-1.0, 1.0]\n",
      "   Thrust command: [0.0, 1.0]\n",
      "‚úÖ Found drone FPV camera\n",
      "üì∑ Available cameras:\n",
      "   Camera 0: track\n",
      "‚úÖ Created 3 renderers\n",
      "‚úÖ Camera system initialized\n",
      "üì∑ Camera system status:\n",
      "   drone_fpv: ID=-1, pos=[-1.00, 0.00, 0.50]\n",
      "‚úÖ Rendering system initialized (OpenCV: ‚úÖ)\n",
      "‚úÖ Reward calculator initialized\n",
      "üéØ Reward components:\n",
      "   height_maintenance: weight=-0.100 - Reward for maintaining target altitude\n",
      "   stability: weight=-0.010 - Reward for low angular velocities\n",
      "   action_penalty: weight=-0.001 - Penalty for large control actions\n",
      "   crash_penalty: weight=1.000 - Large penalty for crashing\n",
      "   progress: weight=0.100 - Reward for forward progress\n",
      "   efficiency: weight=0.050 - Bonus for efficient movement\n",
      "‚úÖ CrazyFlie Environment initialized\n",
      "üìä Observation space: Dict('image': Box(0, 255, (720, 1280, 3), uint8), 'state': Box([-10.        -10.          0.         -5.         -5.         -5.\n",
      "  -3.1415927  -3.1415927  -3.1415927 -10.        -10.        -10.       ], [10.        10.        10.         5.         5.         5.\n",
      "  3.1415927  3.1415927  3.1415927 10.        10.        10.       ], (12,), float32))\n",
      "üéÆ Action space: Box([-1. -1. -1.  0.], 1.0, (4,), float32)\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "üß† CNN Input - Image: (3, 720, 1280), State: 12D\n",
      "üß† CNN output: 2048D\n",
      "üß† Output features: 256D\n",
      "üñ•Ô∏è Live rendering enabled (every 500 steps)\n",
      "Logging to logs/ppo_drone_20251001_151324\\PPO_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0ad2ce6c5b4ba3be5ff80e1adc41d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ratan.Bunkar\\Learning\\general\\rl-agent\\Drone-UAV\\.conda\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x000001C886D47F40> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000001C886D47D30>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üñ•Ô∏è MuJoCo viewer launched\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üñ•Ô∏è MuJoCo viewer launched\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üñ•Ô∏è Rendering enabled - viewer should be visible\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üñ•Ô∏è Rendering enabled - viewer should be visible\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model, results = train_ppo(config, verbose=True)\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Model saved to: {results['final_model_path']}\")\n",
    "print(f\"Logs available at: {results['log_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Trained Model\n",
    "\n",
    "Test the trained model on several episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model (will render first 3 episodes)\n",
    "avg_reward, metrics = test_model(\n",
    "    model_path=results['final_model_path'],\n",
    "    algorithm=\"PPO\",\n",
    "    num_episodes=10,\n",
    "    render=True\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Test Results:\")\n",
    "print(f\"  Average Reward: {metrics['avg_reward']:.2f}\")\n",
    "print(f\"  Std Dev: {metrics['std_reward']:.2f}\")\n",
    "print(f\"  Success Rate: {metrics['success_rate']:.1%}\")\n",
    "print(f\"  Average Episode Length: {metrics['avg_length']:.1f} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View Training Logs with TensorBoard\n",
    "\n",
    "Run this cell to view training metrics in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Launch TensorBoard\n",
    "%tensorboard --logdir logs/\n",
    "\n",
    "# Alternatively, run this in terminal:\n",
    "# tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Episode Trajectory (Optional)\n",
    "\n",
    "Visualize the drone's trajectory during an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Get trajectory data\n",
    "trajectory = visualize_episode(\n",
    "    model_path=results['final_model_path'],\n",
    "    algorithm=\"PPO\",\n",
    "    max_steps=500\n",
    ")\n",
    "\n",
    "# Extract positions\n",
    "positions = [t['position'] for t in trajectory]\n",
    "positions = np.array(positions)\n",
    "\n",
    "# Plot 3D trajectory\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(positions[:, 0], positions[:, 1], positions[:, 2], 'b-', linewidth=2, alpha=0.7)\n",
    "ax.scatter(positions[0, 0], positions[0, 1], positions[0, 2], c='g', s=100, label='Start')\n",
    "ax.scatter(positions[-1, 0], positions[-1, 1], positions[-1, 2], c='r', s=100, label='End')\n",
    "\n",
    "ax.set_xlabel('X Position (m)')\n",
    "ax.set_ylabel('Y Position (m)')\n",
    "ax.set_zlabel('Z Position (m)')\n",
    "ax.set_title('Drone Flight Trajectory')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Episode completed in {len(trajectory)} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train SAC Agent (Alternative)\n",
    "\n",
    "If you want to try SAC instead of PPO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure for SAC\n",
    "sac_config = TrainingConfig(\n",
    "    algorithm=\"SAC\",\n",
    "    total_timesteps=100_000,\n",
    "    num_envs=4,\n",
    "    learning_rate=3e-4,\n",
    "    batch_size=256,\n",
    "    buffer_size=100_000\n",
    ")\n",
    "\n",
    "# Train SAC\n",
    "sac_model, sac_results = train_sac(sac_config, verbose=True)\n",
    "\n",
    "# Test SAC\n",
    "test_model(\n",
    "    model_path=sac_results['final_model_path'],\n",
    "    algorithm=\"SAC\",\n",
    "    num_episodes=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Longer training**: Increase `total_timesteps` to 500K or 1M for better performance\n",
    "- **Hyperparameter tuning**: Experiment with learning rate, batch size, etc.\n",
    "- **Custom rewards**: Modify the environment reward function\n",
    "- **Different algorithms**: Try SAC, A2C, or custom algorithms\n",
    "- **Advanced features**: Add curriculum learning, domain randomization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
